# The code below was generated by AI; see [X] in Use_of_AI.md.
from fastapi import FastAPI, UploadFile, File, HTTPException, Request
from fastapi.responses import JSONResponse
from PIL import Image
import numpy as np
import io
import os
import tensorflow as tf

# ---- Config ----
MODEL_PATH = os.environ.get("MODEL_PATH", "/code/results/best_model.h5")
IMG_SIZE = (128, 128)  # must match training
INPUT_SHAPE = (128, 128, 3)

# IMPORTANT: With Keras' flow_from_directory, class indices are alphabetical.
# Your folders are: 'damage' and 'no_damage' -> {'damage': 0, 'no_damage': 1}
# Your models use a single-unit sigmoid (0/1). So:
#   0 -> 'damage', 1 -> 'no_damage'
INDEX_TO_LABEL = {0: "damage", 1: "no_damage"}

# ---- Load model once at startup ----
try:
    model = tf.keras.models.load_model(MODEL_PATH)
except Exception as e:
    # If model can't be loaded, surface a clear error on /summary
    model = None
    model_load_error = str(e)
else:
    model_load_error = None

app = FastAPI(title="COE379L Project 02 Inference Server")


def _prep_image_bytes(raw: bytes) -> np.ndarray:
    """Convert raw image bytes to a model-ready (1, H, W, C) float32 array."""
    if not raw:
        raise ValueError("Empty request body.")
    try:
        img = Image.open(io.BytesIO(raw)).convert("RGB")
    except Exception as e:
        raise ValueError(f"Unable to read image: {e}")

    img = img.resize(IMG_SIZE)  # center/warp resize; matches training
    arr = np.asarray(img, dtype=np.float32) / 255.0
    arr = np.expand_dims(arr, axis=0)  # (1, H, W, C)
    return arr


def _predict(image_arr: np.ndarray) -> str:
    """
    Supports both:
      - Binary sigmoid: output shape (1, 1)
      - Two-logit softmax: output shape (1, 2)
    Returns the project-required label string: "damage" or "no_damage".
    """
    pred = model.predict(image_arr, verbose=0)

    # Sigmoid case: [[p]] with p in [0,1]
    if pred.ndim == 2 and pred.shape[1] == 1:
        cls_idx = 1 if pred[0, 0] >= 0.5 else 0

    # Softmax case: [[p0, p1]]
    elif pred.ndim == 2 and pred.shape[1] == 2:
        cls_idx = int(np.argmax(pred[0]))

    else:
        raise RuntimeError(f"Unexpected model output shape: {pred.shape}")

    return INDEX_TO_LABEL[int(cls_idx)]


# -------------------- Endpoints --------------------

@app.get("/summary")
def summary():
    """
    REQUIRED by spec:
    - Must accept GET /summary
    - Must return JSON (object), e.g. { ... }
    """
    info = {
        "model_name": getattr(model, "name", "unknown") if model else None,
        "status": "ready" if model and not model_load_error else "error",
        "input_shape": INPUT_SHAPE,
    }
    if model_load_error:
        info["error"] = model_load_error
        return JSONResponse(status_code=500, content=info)
    return info


@app.post("/inference")
async def inference(
    request: Request,
    file: UploadFile | None = File(default=None),
    image: UploadFile | None = File(default=None),
):
    """
    Handles both multipart (file/image) and raw binary uploads.
    Compatible with grader.py and manual curl.
    """
    if model is None:
        raise HTTPException(status_code=500, detail=f"Model not loaded: {model_load_error}")

    try:
        raw = None

        # Case 1: Grader sends "image"
        if image is not None:
            raw = await image.read()

        # Case 2: Manual test or alternate clients send "file"
        elif file is not None:
            raw = await file.read()

        # Case 3: Raw bytes directly (no multipart)
        else:
            raw = await request.body()

        if not raw:
            raise ValueError("No image data received in request body.")

        # Convert image bytes to numpy array
        image_arr = _prep_image_bytes(raw)
        pred_label = _predict(image_arr)

        return {"prediction": pred_label}

    except Exception as e:
        print(f"[ERROR] Inference failed: {e}")
        raise HTTPException(status_code=500, detail=f"Inference failed: {e}")
